"""
FocusAI Insights API Router
Handles news card listing and detail endpoints.
"""
from fastapi import APIRouter, Query, HTTPException
from pydantic import BaseModel
from typing import List, Dict
from collections import defaultdict
import time

from models import InsightCard, InsightListResponse
from storage import storage
from config import PROFESSIONS

router = APIRouter(prefix="/api/insights", tags=["Insights"])


# ============================================
# ç¼“å­˜ç®¡ç†å™¨ - å‡å°‘ API è°ƒç”¨ï¼ŒåŠ å¿«å“åº”é€Ÿåº¦
# ============================================
class ContentCache:
    """
    å·¥å…·/æ¡ˆä¾‹ç¼“å­˜ç®¡ç†å™¨
    - æ¯æ¬¡ API è°ƒç”¨è·å– 12 æ¡ï¼Œæ˜¾ç¤º 6 æ¡ï¼Œç¼“å­˜ 6 æ¡
    - åˆ·æ–°æ—¶å…ˆä»ç¼“å­˜å–ï¼Œç¼“å­˜ç©ºäº†å†è°ƒç”¨ API
    - ç¼“å­˜ 30 åˆ†é’Ÿè¿‡æœŸ
    """
    DISPLAY_COUNT = 6      # æ¯æ¬¡æ˜¾ç¤ºæ•°é‡
    FETCH_COUNT = 18       # æ¯æ¬¡è·å–æ•°é‡ï¼ˆæ˜¾ç¤º6æ¡ + ç¼“å­˜12æ¡ = æ”¯æŒåˆ°2æ¬¡åˆ·æ–°ï¼‰
    CACHE_TTL = 1800       # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆ30åˆ†é’Ÿï¼‰
    
    def __init__(self):
        # ç¼“å­˜ç»“æ„: {profession: {"items": [...], "timestamp": ..., "seed": ...}}
        self.tools_cache: Dict[str, dict] = {}
        self.cases_cache: Dict[str, dict] = {}
    
    def _is_expired(self, cache_entry: dict) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
        if not cache_entry:
            return True
        return time.time() - cache_entry.get("timestamp", 0) > self.CACHE_TTL
    
    def get_tools(self, profession: str) -> tuple[list, bool]:
        """
        è·å–å·¥å…·ç¼“å­˜
        è¿”å›: (items, need_fetch) - ç¼“å­˜é¡¹å’Œæ˜¯å¦éœ€è¦è°ƒç”¨ API
        """
        cache = self.tools_cache.get(profession)
        
        # ç¼“å­˜ä¸å­˜åœ¨æˆ–è¿‡æœŸ
        if self._is_expired(cache):
            return [], True
        
        items = cache.get("items", [])
        
        # ç¼“å­˜ä¸è¶³ï¼Œéœ€è¦é‡æ–°è·å–
        if len(items) < self.DISPLAY_COUNT:
            return [], True
        
        # å–å‡ºå‰ 6 æ¡ï¼Œå‰©ä½™çš„ä¿ç•™åœ¨ç¼“å­˜
        result = items[:self.DISPLAY_COUNT]
        cache["items"] = items[self.DISPLAY_COUNT:]
        
        print(f"ğŸ“¦ [Tools] ç¼“å­˜å‘½ä¸­! è¿”å› {len(result)} æ¡ï¼Œå‰©ä½™ç¼“å­˜ {len(cache['items'])} æ¡")
        return result, False
    
    def set_tools(self, profession: str, items: list, seed: int):
        """è®¾ç½®å·¥å…·ç¼“å­˜"""
        self.tools_cache[profession] = {
            "items": items,
            "timestamp": time.time(),
            "seed": seed
        }
        print(f"ğŸ“¥ [Tools] ç¼“å­˜æ›´æ–°! å­˜å…¥ {len(items)} æ¡")
    
    def get_cases(self, profession: str) -> tuple[list, bool]:
        """è·å–æ¡ˆä¾‹ç¼“å­˜"""
        cache = self.cases_cache.get(profession)
        
        if self._is_expired(cache):
            return [], True
        
        items = cache.get("items", [])
        
        if len(items) < self.DISPLAY_COUNT:
            return [], True
        
        result = items[:self.DISPLAY_COUNT]
        cache["items"] = items[self.DISPLAY_COUNT:]
        
        print(f"ğŸ“¦ [Cases] ç¼“å­˜å‘½ä¸­! è¿”å› {len(result)} æ¡ï¼Œå‰©ä½™ç¼“å­˜ {len(cache['items'])} æ¡")
        return result, False
    
    def set_cases(self, profession: str, items: list, seed: int):
        """è®¾ç½®æ¡ˆä¾‹ç¼“å­˜"""
        self.cases_cache[profession] = {
            "items": items,
            "timestamp": time.time(),
            "seed": seed
        }
        print(f"ğŸ“¥ [Cases] ç¼“å­˜æ›´æ–°! å­˜å…¥ {len(items)} æ¡")
    
    def get_next_seed(self, cache_type: str, profession: str) -> int:
        """è·å–ä¸‹ä¸€ä¸ª seed å€¼"""
        cache = self.tools_cache if cache_type == "tools" else self.cases_cache
        entry = cache.get(profession, {})
        return entry.get("seed", -1) + 1


# å…¨å±€ç¼“å­˜å®ä¾‹
content_cache = ContentCache()

# æ•°æ®å­˜å‚¨è·¯å¾„
from pathlib import Path
import json
DATA_DIR = Path(__file__).parent.parent / "data"


def _save_user_daily_content(user_id: str, content_type: str, date: str, items: list):
    """ä¿å­˜å…è´¹ç”¨æˆ·çš„æ¯æ—¥å†…å®¹ï¼ˆç”¨äºé”å®šï¼‰"""
    try:
        user_dir = DATA_DIR / "user_daily" / content_type
        user_dir.mkdir(parents=True, exist_ok=True)
        user_file = user_dir / f"{user_id}_{date}.json"
        
        with open(user_file, 'w', encoding='utf-8') as f:
            json.dump({
                "user_id": user_id,
                "date": date,
                "items": items,
                "created_at": time.strftime("%Y-%m-%d %H:%M:%S")
            }, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ [{content_type}] ä¿å­˜å…è´¹ç”¨æˆ· {user_id} çš„ä»Šæ—¥å†…å®¹")
    except Exception as e:
        print(f"ä¿å­˜ç”¨æˆ·æ¯æ—¥å†…å®¹å¤±è´¥: {e}")


@router.get("", response_model=InsightListResponse)
async def list_insights(
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(20, ge=1, le=50, description="Items per page"),
):
    """
    Get a list of AI insight cards.
    Returns cards from local storage, ordered by newest first.
    """
    offset = (page - 1) * page_size
    items = await storage.get_insights(limit=page_size, offset=offset)
    total = await storage.get_insights_count()
    
    # è½¬æ¢ä¸º InsightCard å¯¹è±¡
    cards = [InsightCard(**item) for item in items]
    
    return InsightListResponse(
        items=cards,
        total=total,
        page=page,
        page_size=page_size
    )


@router.get("/mock", response_model=List[InsightCard])
async def get_mock_insights(
    profession: str = Query("other", description="User profession for mock data")
):
    """
    Get mock insight cards for development/demo.
    No database required.
    """
    profession_display = PROFESSIONS.get(profession, "èŒåœºäººå£«")
    
    mock_data = [
        InsightCard(
            id="mock-1",
            title="DeepSeek V3 å‘å¸ƒï¼šæ€§èƒ½è¶…è¶Š GPT-4",
            tags=["#DeepSeek", "#å¤§æ¨¡å‹", "#å¼€æº"],
            summary="æ·±åº¦æ±‚ç´¢å‘å¸ƒ DeepSeek V3ï¼Œåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Š GPT-4ï¼Œä¸”å®Œå…¨å¼€æºå…è´¹ã€‚æ¨¡å‹æ”¯æŒ 128K ä¸Šä¸‹æ–‡ï¼Œæ¨ç†é€Ÿåº¦æå‡ 3 å€ã€‚",
            impact=f"ä½œä¸º{profession_display}ï¼Œä½ å¯ä»¥ç”¨ DeepSeek V3 æ¥è¾…åŠ©æ—¥å¸¸å·¥ä½œã€‚å®ƒçš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ç‰¹åˆ«é€‚åˆå¤„ç†é•¿æ–‡æ¡£ã€ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šã€‚å»ºè®®ä½ å…ˆåœ¨ç®€å•ä»»åŠ¡ä¸Šè¯•ç”¨ï¼Œé€æ­¥æ›¿ä»£éƒ¨åˆ†é‡å¤æ€§å·¥ä½œã€‚",
            prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ã€‚è¯·å¸®æˆ‘åˆ†æä»¥ä¸‹å†…å®¹ï¼Œå¹¶ç»™å‡ºç»“æ„åŒ–çš„æ€»ç»“å’Œå¯è¡Œçš„å»ºè®®ï¼š\n\n[åœ¨æ­¤ç²˜è´´ä½ çš„å†…å®¹]",
            url="https://www.deepseek.com/",
            timestamp="2024-12-03"
        ),
        InsightCard(
            id="mock-2",
            title="Midjourney V6.1 é‡å¤§æ›´æ–°ï¼šæ–‡å­—æ¸²æŸ“èƒ½åŠ›çªç ´",
            tags=["#Midjourney", "#AIç»˜ç”»", "#è®¾è®¡"],
            summary="Midjourney å‘å¸ƒ V6.1 ç‰ˆæœ¬ï¼Œé¦–æ¬¡å®ç°é«˜è´¨é‡æ–‡å­—æ¸²æŸ“ï¼Œå¯ä»¥ç›´æ¥åœ¨å›¾ç‰‡ä¸­ç”Ÿæˆæ¸…æ™°å¯è¯»çš„æ–‡å­—ï¼ŒåŒæ—¶å›¾åƒç”Ÿæˆé€Ÿåº¦æå‡ 2 å€ã€‚",
            impact=f"å¯¹{profession_display}æ¥è¯´ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥å¿«é€Ÿç”Ÿæˆå¸¦æœ‰æ–‡å­—çš„æµ·æŠ¥ã€å°é¢ã€å®£ä¼ å›¾ï¼Œä¸å†éœ€è¦åæœŸ PS åŠ å­—ã€‚éå¸¸é€‚åˆåˆ¶ä½œæ•™å­¦ææ–™ã€è¯¾ä»¶é…å›¾ã€‚",
            prompt="/imagine prompt: æ•™è‚²ä¸»é¢˜æ’ç”»ï¼Œæ¸©é¦¨æ˜äº®çš„æ•™å®¤åœºæ™¯ï¼Œå­¦ç”Ÿè®¤çœŸå­¦ä¹ ï¼Œé»‘æ¿ä¸Šå†™ç€\"çŸ¥è¯†æ”¹å˜å‘½è¿\"ï¼Œæ¸©æš–çš„é˜³å…‰é€è¿‡çª—æˆ· --v 6.1 --ar 16:9",
            url="https://midjourney.com/",
            timestamp="2024-12-02"
        ),
        InsightCard(
            id="mock-3",
            title="Claude 3.5 Sonnet æ–°åŠŸèƒ½ï¼šArtifacts å®æ—¶é¢„è§ˆ",
            tags=["#Claude", "#Anthropic", "#ç¼–ç¨‹"],
            summary="Anthropic ä¸º Claude 3.5 Sonnet æ¨å‡º Artifacts åŠŸèƒ½ï¼Œç”¨æˆ·å¯ä»¥åœ¨å¯¹è¯ä¸­å®æ—¶é¢„è§ˆå’Œè¿è¡Œä»£ç ã€æŸ¥çœ‹ç”Ÿæˆçš„æ–‡æ¡£å’Œå›¾è¡¨ã€‚",
            impact=f"ä½œä¸º{profession_display}ï¼ŒArtifacts åŠŸèƒ½å¯ä»¥å¸®ä½ å¿«é€ŸéªŒè¯æƒ³æ³•ã€‚æ¯”å¦‚è®© Claude ç”Ÿæˆä¸€ä¸ªè¯¾ç¨‹å¤§çº²ï¼Œå®ƒä¼šä»¥ç»“æ„åŒ–çš„æ–¹å¼å‘ˆç°ï¼Œä½ å¯ä»¥ç›´æ¥ç¼–è¾‘å’Œå¯¼å‡ºã€‚",
            prompt="è¯·å¸®æˆ‘è®¾è®¡ä¸€ä¸ªä¸ºæœŸ 4 å‘¨çš„è¯¾ç¨‹å¤§çº²ï¼Œä¸»é¢˜æ˜¯ [ä½ çš„è¯¾ç¨‹ä¸»é¢˜]ã€‚è¦æ±‚ï¼š\n1. æ¯å‘¨ 2 èŠ‚è¯¾ï¼Œæ¯èŠ‚ 1 å°æ—¶\n2. åŒ…å«ç†è®ºè®²è§£å’Œå®è·µç»ƒä¹ \n3. è®¾ç½®è¯¾åä½œä¸šå’Œé˜¶æ®µæ€§æµ‹éªŒ\n4. ä½¿ç”¨ Markdown æ ¼å¼è¾“å‡º",
            url="https://claude.ai/",
            timestamp="2024-12-01"
        ),
        InsightCard(
            id="mock-4",
            title="Sora å³å°†å¼€æ”¾ï¼šAI è§†é¢‘ç”Ÿæˆè¿›å…¥å®ç”¨é˜¶æ®µ",
            tags=["#Sora", "#OpenAI", "#è§†é¢‘ç”Ÿæˆ"],
            summary="OpenAI å®£å¸ƒ Sora å°†äºæœ¬æœˆå‘ ChatGPT Plus ç”¨æˆ·å¼€æ”¾ã€‚Sora å¯ä»¥æ ¹æ®æ–‡å­—æè¿°ç”Ÿæˆæœ€é•¿ 60 ç§’çš„é«˜æ¸…è§†é¢‘ï¼Œæ”¯æŒå¤šç§é£æ ¼ã€‚",
            impact=f"è§†é¢‘å†…å®¹å°†å˜å¾—æ›´å®¹æ˜“åˆ¶ä½œã€‚{profession_display}å¯ä»¥ç”¨å®ƒæ¥åˆ¶ä½œæ•™å­¦æ¼”ç¤ºè§†é¢‘ã€è¯¾ç¨‹å®£ä¼ ç‰‡ã€‚è™½ç„¶ç›®å‰è¿˜ä¸èƒ½å®Œå…¨æ›¿ä»£ä¸“ä¸šåˆ¶ä½œï¼Œä½†è¶³ä»¥åº”å¯¹æ—¥å¸¸éœ€æ±‚ã€‚",
            prompt="Create a 15-second educational video: A friendly animated teacher explaining the concept of [ä½ çš„ä¸»é¢˜] to students. Warm, inviting classroom setting with soft lighting. Professional yet approachable style.",
            url="https://openai.com/sora",
            timestamp="2024-11-30"
        ),
    ]
    
    return mock_data


@router.get("/tools")
async def get_ai_tools(
    profession: str = Query("èŒåœºäººå£«", description="ç”¨æˆ·èŒä¸š"),
    user_id: str = Query("anonymous", description="ç”¨æˆ·ID"),
    force_refresh: bool = Query(False, description="å¼ºåˆ¶åˆ·æ–°ï¼Œè·³è¿‡ç¼“å­˜")
):
    """
    è·å– AI å·¥å…·æ¨è
    - å…è´¹ç”¨æˆ·ï¼šæ¯å¤©åªç”Ÿæˆä¸€æ¬¡ï¼Œä¹‹åè¿”å›ç›¸åŒå†…å®¹
    - ä¸“ä¸šç‰ˆç”¨æˆ·ï¼šå¯ä»¥æ— é™åˆ·æ–°
    """
    from services.ai_processor import ai_processor
    from services.supabase_db import get_premium_status
    from datetime import datetime
    
    today = datetime.now().strftime("%Y-%m-%d")
    
    # æ£€æŸ¥ç”¨æˆ·ä¸“ä¸šç‰ˆçŠ¶æ€ï¼ˆSupabase å¯èƒ½æœªé…ç½®ï¼Œä¼˜é›…é™çº§ï¼‰
    is_premium = False
    try:
        premium_status = get_premium_status(user_id)
        is_premium = premium_status and premium_status.get("expires_at")
        if is_premium:
            from datetime import datetime as dt
            expires_at = dt.fromisoformat(premium_status["expires_at"].replace("Z", "+00:00"))
            is_premium = expires_at > dt.now(expires_at.tzinfo)
    except Exception as e:
        print(f"âš ï¸ [Tools] è·å–ä¸“ä¸šç‰ˆçŠ¶æ€å¤±è´¥ï¼ˆSupabaseå¯èƒ½æœªé…ç½®ï¼‰: {e}")
        is_premium = False
    
    # å…è´¹ç”¨æˆ·ï¼šæ£€æŸ¥ä»Šæ—¥æ˜¯å¦å·²æœ‰å†…å®¹
    if not is_premium and not force_refresh:
        user_tools_dir = DATA_DIR / "user_daily" / "tools"
        user_tools_file = user_tools_dir / f"{user_id}_{today}.json"
        
        if user_tools_file.exists():
            try:
                with open(user_tools_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    print(f"ğŸ“¦ [Tools] å…è´¹ç”¨æˆ· {user_id} è¿”å›ä»Šæ—¥å·²æœ‰å†…å®¹")
                    return {
                        "items": data.get("items", []),
                        "profession": profession,
                        "source": "user_daily_cache",
                        "cached": True
                    }
            except Exception as e:
                print(f"è¯»å–ç”¨æˆ·å·¥å…·ç¼“å­˜å¤±è´¥: {e}")
    
    # ä¸“ä¸šç‰ˆæˆ–é¦–æ¬¡è®¿é—®ï¼šä»ç¼“å­˜è·å–æˆ–ç”Ÿæˆæ–°å†…å®¹
    if not force_refresh:
        cached_items, need_fetch = content_cache.get_tools(profession)
        if not need_fetch:
            # å…è´¹ç”¨æˆ·é¦–æ¬¡è®¿é—®ï¼Œä¿å­˜ä»Šæ—¥å†…å®¹
            if not is_premium:
                _save_user_daily_content(user_id, "tools", today, cached_items)
            return {
                "items": cached_items, 
                "profession": profession, 
                "source": "cache",
                "cached": True
            }
    
    # ç¼“å­˜ä¸è¶³ï¼Œè°ƒç”¨ API
    try:
        seed = content_cache.get_next_seed("tools", profession)
        print(f"ğŸ”„ [Tools] ç¼“å­˜ä¸è¶³ï¼Œè°ƒç”¨ API (seed={seed})")
        
        tools = await ai_processor.search_and_recommend_tools(
            profession, 
            refresh_seed=seed,
            result_count=ContentCache.FETCH_COUNT
        )
        
        display_items = tools[:ContentCache.DISPLAY_COUNT]
        cache_items = tools[ContentCache.DISPLAY_COUNT:]
        
        if cache_items:
            content_cache.set_tools(profession, cache_items, seed)
        
        # å…è´¹ç”¨æˆ·é¦–æ¬¡è®¿é—®ï¼Œä¿å­˜ä»Šæ—¥å†…å®¹
        if not is_premium:
            _save_user_daily_content(user_id, "tools", today, display_items)
        
        return {
            "items": display_items, 
            "profession": profession, 
            "source": "web_search",
            "cached": False,
            "total_fetched": len(tools)
        }
    except Exception as e:
        print(f"Error searching tools: {e}")
        # é™çº§åˆ°é™æ€æ•°æ®
        import json
        from pathlib import Path
        tools_file = Path(__file__).parent.parent / "data" / "ai_tools.json"
        if tools_file.exists():
            with open(tools_file, 'r', encoding='utf-8') as f:
                return {"items": json.load(f)[:6], "source": "fallback"}
        return {"items": [], "error": str(e)}


@router.get("/cases")
async def get_ai_cases(
    profession: str = Query("èŒåœºäººå£«", description="ç”¨æˆ·èŒä¸š"),
    user_id: str = Query("anonymous", description="ç”¨æˆ·ID"),
    force_refresh: bool = Query(False, description="å¼ºåˆ¶åˆ·æ–°ï¼Œè·³è¿‡ç¼“å­˜")
):
    """
    è·å– AI å®æˆ˜æ¡ˆä¾‹
    - å…è´¹ç”¨æˆ·ï¼šæ¯å¤©åªç”Ÿæˆä¸€æ¬¡ï¼Œä¹‹åè¿”å›ç›¸åŒå†…å®¹
    - ä¸“ä¸šç‰ˆç”¨æˆ·ï¼šå¯ä»¥æ— é™åˆ·æ–°
    """
    from services.ai_processor import ai_processor
    from services.supabase_db import get_premium_status
    from datetime import datetime
    
    today = datetime.now().strftime("%Y-%m-%d")
    
    # æ£€æŸ¥ç”¨æˆ·ä¸“ä¸šç‰ˆçŠ¶æ€ï¼ˆSupabase å¯èƒ½æœªé…ç½®ï¼Œä¼˜é›…é™çº§ï¼‰
    is_premium = False
    try:
        premium_status = get_premium_status(user_id)
        is_premium = premium_status and premium_status.get("expires_at")
        if is_premium:
            from datetime import datetime as dt
            expires_at = dt.fromisoformat(premium_status["expires_at"].replace("Z", "+00:00"))
            is_premium = expires_at > dt.now(expires_at.tzinfo)
    except Exception as e:
        print(f"âš ï¸ [Cases] è·å–ä¸“ä¸šç‰ˆçŠ¶æ€å¤±è´¥ï¼ˆSupabaseå¯èƒ½æœªé…ç½®ï¼‰: {e}")
        is_premium = False
    
    # å…è´¹ç”¨æˆ·ï¼šæ£€æŸ¥ä»Šæ—¥æ˜¯å¦å·²æœ‰å†…å®¹
    if not is_premium and not force_refresh:
        user_cases_dir = DATA_DIR / "user_daily" / "cases"
        user_cases_file = user_cases_dir / f"{user_id}_{today}.json"
        
        if user_cases_file.exists():
            try:
                with open(user_cases_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    print(f"ğŸ“¦ [Cases] å…è´¹ç”¨æˆ· {user_id} è¿”å›ä»Šæ—¥å·²æœ‰å†…å®¹")
                    return {
                        "items": data.get("items", []),
                        "profession": profession,
                        "source": "user_daily_cache",
                        "cached": True
                    }
            except Exception as e:
                print(f"è¯»å–ç”¨æˆ·æ¡ˆä¾‹ç¼“å­˜å¤±è´¥: {e}")
    
    # ä¸“ä¸šç‰ˆæˆ–é¦–æ¬¡è®¿é—®ï¼šä»ç¼“å­˜è·å–æˆ–ç”Ÿæˆæ–°å†…å®¹
    if not force_refresh:
        cached_items, need_fetch = content_cache.get_cases(profession)
        if not need_fetch:
            # å…è´¹ç”¨æˆ·é¦–æ¬¡è®¿é—®ï¼Œä¿å­˜ä»Šæ—¥å†…å®¹
            if not is_premium:
                _save_user_daily_content(user_id, "cases", today, cached_items)
            return {
                "items": cached_items, 
                "profession": profession, 
                "source": "cache",
                "cached": True
            }
    
    # ç¼“å­˜ä¸è¶³ï¼Œè°ƒç”¨ API
    try:
        seed = content_cache.get_next_seed("cases", profession)
        print(f"ğŸ”„ [Cases] ç¼“å­˜ä¸è¶³ï¼Œè°ƒç”¨ API (seed={seed})")
        
        cases = await ai_processor.search_and_recommend_cases(
            profession, 
            refresh_seed=seed,
            result_count=ContentCache.FETCH_COUNT
        )
        
        display_items = cases[:ContentCache.DISPLAY_COUNT]
        cache_items = cases[ContentCache.DISPLAY_COUNT:]
        
        if cache_items:
            content_cache.set_cases(profession, cache_items, seed)
        
        # å…è´¹ç”¨æˆ·é¦–æ¬¡è®¿é—®ï¼Œä¿å­˜ä»Šæ—¥å†…å®¹
        if not is_premium:
            _save_user_daily_content(user_id, "cases", today, display_items)
        
        return {
            "items": display_items, 
            "profession": profession, 
            "source": "web_search",
            "cached": False,
            "total_fetched": len(cases)
        }
    except Exception as e:
        print(f"Error searching cases: {e}")
        # é™çº§åˆ°é™æ€æ•°æ®
        import json
        from pathlib import Path
        cases_file = Path(__file__).parent.parent / "data" / "ai_cases.json"
        if cases_file.exists():
            with open(cases_file, 'r', encoding='utf-8') as f:
                return {"items": json.load(f)[:6], "source": "fallback"}
        return {"items": [], "error": str(e)}


# ============================================
# ç”Ÿæˆä»Šæ—¥æ–°é—» - ä½¿ç”¨ Tavily æœç´¢æœ€æ–° AI èµ„è®¯
# ============================================
@router.get("/generate")
async def generate_daily_news(
    profession: str = Query("èŒåœºäººå£«", description="ç”¨æˆ·èŒä¸š"),
    user_id: str = Query("anonymous", description="ç”¨æˆ·ID")
):
    """
    ç”Ÿæˆä»Šæ—¥ AI æ–°é—» - æœç´¢æœ€æ–°èµ„è®¯å¹¶ç”¨ AI ç”Ÿæˆä¸ªæ€§åŒ–è§£è¯»
    """
    from services.ai_processor import ai_processor
    from services.content_safety import validate_profession, check_rate_limit, is_user_blocked
    import uuid
    import asyncio
    from tavily import TavilyClient
    from config import get_settings
    
    # å®‰å…¨æ£€æµ‹
    if is_user_blocked(user_id):
        raise HTTPException(status_code=403, detail="è´¦å·å·²è¢«é™åˆ¶ï¼Œè¯·è”ç³»ç®¡ç†å‘˜")
    
    rate_ok, rate_msg = check_rate_limit(user_id)
    if not rate_ok:
        raise HTTPException(status_code=429, detail=rate_msg)
    
    valid, msg = validate_profession(profession, user_id)
    if not valid:
        raise HTTPException(status_code=400, detail=msg)
    
    print(f"ğŸ”„ [News] å¼€å§‹ç”Ÿæˆä»Šæ—¥æ–°é—»ï¼ŒèŒä¸š: {profession}")
    
    try:
        # è·å– Tavily API Key
        keys = get_settings().get_tavily_keys()
        if not keys:
            raise Exception("æœªé…ç½® Tavily API Key")
        
        api_key = keys[0]
        
        # æœç´¢æœ€æ–° AI æ–°é—» - ä½¿ç”¨æ›´ç²¾ç¡®çš„æ—¶é—´å…³é”®è¯
        from datetime import datetime, timedelta
        today = datetime.now()
        date_str = today.strftime("%Yå¹´%mæœˆ")  # å¦‚ "2025å¹´12æœˆ"
        yesterday = (today - timedelta(days=1)).strftime("%mæœˆ%dæ—¥")
        
        # å›½å†…æ–°é—»æœç´¢å…³é”®è¯
        cn_query_templates = [
            f"AIäººå·¥æ™ºèƒ½ æœ€æ–°æ–°é—» {date_str}",
            f"AIå¤§æ¨¡å‹ å‘å¸ƒ {date_str}",
            f"ChatGPT Claude Gemini æ›´æ–° {date_str}",
            f"AI Agent æ™ºèƒ½ä½“ æœ€æ–° {date_str}",
            f"DeepSeek å­—èŠ‚è±†åŒ… ç™¾åº¦æ–‡å¿ƒ æ–°é—»",
        ]
        # å›½é™…æ–°é—»æœç´¢å…³é”®è¯
        en_query_templates = [
            "AI artificial intelligence news today",
            "OpenAI Anthropic Google AI latest",
            "ChatGPT Claude Gemini update",
            "AI breakthrough technology news",
            "machine learning deep learning news",
        ]
        
        import random
        cn_query = random.choice(cn_query_templates)
        en_query = random.choice(en_query_templates)
        print(f"   å›½å†…æœç´¢: {cn_query}")
        print(f"   å›½é™…æœç´¢: {en_query}")
        
        # ä¸­æ–‡ç½‘ç«™ï¼ˆä¸å«çŸ¥ä¹ï¼‰
        cn_domains = [
            "36kr.com", "sspai.com", "juejin.cn",
            "mp.weixin.qq.com", "csdn.net", "jiqizhixin.com",
            "pingwest.com", "geekpark.net", "leiphone.com",
        ]
        # å›½é™…ç§‘æŠ€åª’ä½“
        en_domains = [
            "theverge.com", "techcrunch.com", "wired.com",
            "arstechnica.com", "venturebeat.com", "zdnet.com",
            "cnet.com", "engadget.com", "thenextweb.com",
            "reuters.com", "bbc.com", "nature.com", "ieee.org",
            "openai.com", "anthropic.com", "huggingface.co",
            "towardsdatascience.com", "medium.com"
        ]
        
        client = TavilyClient(api_key=api_key)
        
        # æœç´¢å›½å†…æ–°é—»
        cn_response = await asyncio.to_thread(
            client.search,
            query=cn_query,
            search_depth="advanced",
            max_results=15,
            include_domains=cn_domains,
            days=3
        )
        cn_results = cn_response.get("results", [])
        
        # æœç´¢å›½é™…æ–°é—»
        en_response = await asyncio.to_thread(
            client.search,
            query=en_query,
            search_depth="advanced",
            max_results=15,
            include_domains=en_domains,
            days=3
        )
        en_results = en_response.get("results", [])
        
        # åˆå¹¶ç»“æœ
        results = cn_results + en_results
        
        if not results:
            raise Exception("æœªæœç´¢åˆ°æœ‰æ•ˆæ–°é—»")
        
        print(f"   æœç´¢åˆ° {len(results)} æ¡ç»“æœ")
        
        # æ ¼å¼åŒ–æœç´¢ç»“æœä¾› AI å¤„ç†ï¼ŒåŒ…å«å‘å¸ƒæ—¥æœŸ
        search_context = ""
        for i, res in enumerate(results):
            pub_date = res.get('published_date', '') or res.get('publishedDate', '') or 'æœªçŸ¥'
            search_context += f"[{i+1}] æ ‡é¢˜: {res.get('title', '')}\né“¾æ¥: {res.get('url', '')}\nå‘å¸ƒæ—¶é—´: {pub_date}\næ‘˜è¦: {res.get('content', '')}\n\n"
        
        # AI ç”Ÿæˆç»“æ„åŒ–æ–°é—»å¡ç‰‡
        today_str = today.strftime("%Yå¹´%mæœˆ%dæ—¥")
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI è¡Œä¸šåˆ†æå¸ˆã€‚ä»Šå¤©æ˜¯ {today_str}ã€‚
æˆ‘ä¸ºä½ æœé›†äº†å›½å†…å¤–æœ€æ–°çš„ AI ç›¸å…³æ–°é—»ï¼Œè¯·ä»”ç»†é˜…è¯»å¹¶ä¸º"{profession}"ç”Ÿæˆ 10 æ¡é«˜è´¨é‡çš„ AI è¡Œä¸šæ´å¯Ÿå¡ç‰‡ã€‚

æœç´¢ç»“æœï¼š
{search_context}

è¦æ±‚ï¼š
1. ã€æ—¶æ•ˆæ€§ä¼˜å…ˆã€‘ä¼˜å…ˆé€‰æ‹©å‘å¸ƒæ—¶é—´æœ€è¿‘çš„æ–°é—»
2. ã€å›½å†…å¤–å¹³è¡¡ã€‘å¿…é¡»åŒæ—¶åŒ…å«å›½å†…å’Œå›½é™…æ–°é—»ï¼Œå¤§çº¦å„å ä¸€åŠ
3. æ¯æ¡æ´å¯Ÿéƒ½å¿…é¡»åŸºäºçœŸå®çš„æœç´¢ç»“æœï¼Œä¸è¦ç¼–é€ 
4. ã€é‡è¦ã€‘æ¯æ¡æ–°é—»å¿…é¡»ä½¿ç”¨ä¸åŒçš„åŸæ–‡é“¾æ¥(url)ï¼Œç»å¯¹ä¸èƒ½é‡å¤ï¼
5. url å­—æ®µå¿…é¡»ç›´æ¥å¤åˆ¶æœç´¢ç»“æœä¸­çš„â€œé“¾æ¥â€ï¼Œä¸è¦ä¿®æ”¹
6. ä¸ºæ¯æ¡æ–°é—»ç”Ÿæˆï¼šæ ‡é¢˜ã€æ ‡ç­¾ã€æ‘˜è¦ã€å¯¹è¯¥èŒä¸šçš„å…·ä½“å½±å“ã€å¯ç›´æ¥ä½¿ç”¨çš„ Prompt
7. æ‘˜è¦è¦ç®€æ´æœ‰ä¿¡æ¯é‡ï¼ˆ50-100å­—ï¼‰ï¼Œè‹±æ–‡æ–°é—»è¯·ç¿»è¯‘æˆä¸­æ–‡
8. å½±å“åˆ†æè¦é’ˆå¯¹ {profession} è¿™ä¸ªèŒä¸šå…·ä½“åŒ–
9. Prompt è¦å®ç”¨ï¼Œå¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼š
[
  {{
    "id": "news-1",
    "title": "æ–°é—»æ ‡é¢˜ï¼ˆå¯ä»¥é‡æ–°ç»„ç»‡è¯­è¨€ï¼Œä½†è¦å¿ äºåŸæ„ï¼‰",
    "tags": ["#æ ‡ç­¾1", "#æ ‡ç­¾2", "#æ ‡ç­¾3"],
    "summary": "æ–°é—»æ‘˜è¦ï¼Œç®€æ´æœ‰ä¿¡æ¯é‡ï¼ŒåŒ…å«æ—¶é—´ä¿¡æ¯",
    "impact": "å¯¹{profession}çš„å…·ä½“å½±å“å’Œå»ºè®®",
    "prompt": "å¯ç›´æ¥ä½¿ç”¨çš„ Prompt ç¤ºä¾‹",
    "url": "ç›´æ¥å¤åˆ¶æœç´¢ç»“æœä¸­çš„é“¾æ¥ï¼Œä¸è¦ä¿®æ”¹"
  }}
]

åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        import json as json_module
        response = await asyncio.to_thread(
            ai_processor.client.chat.completions.create,
            model=ai_processor.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=6000  # å¢åŠ ä»¥æ”¯æŒ10æ¡æ–°é—»
        )
        
        content = response.choices[0].message.content.strip()
        
        # æå– JSON
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].strip()
        
        start = content.find('[')
        end = content.rfind(']')
        if start != -1 and end != -1:
            content = content[start:end+1]
        
        news_items = json_module.loads(content)
        original_count = len(news_items)
        
        # å»é™¤é‡å¤ URL çš„æ–°é—»
        seen_urls = set()
        unique_items = []
        for item in news_items:
            url = item.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_items.append(item)
        news_items = unique_items
        
        if len(news_items) < original_count:
            print(f"âš ï¸ [News] å»é™¤äº† {original_count - len(news_items)} æ¡é‡å¤ URL çš„æ–°é—»")
        
        # æ·»åŠ å”¯ä¸€ ID å’Œæ—¶é—´æˆ³
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y-%m-%d")
        for item in news_items:
            item['id'] = f"news-{uuid.uuid4().hex[:8]}"
            item['timestamp'] = timestamp
        
        print(f"âœ… [News] æˆåŠŸç”Ÿæˆ {len(news_items)} æ¡æ–°é—»")
        
        # ä¿å­˜ç”¨æˆ·çš„æ–°é—»åˆ°æ–‡ä»¶ï¼ˆä¾›åˆ†äº«ä½¿ç”¨ï¼‰
        try:
            user_news_dir = DATA_DIR / "user_news"
            user_news_dir.mkdir(parents=True, exist_ok=True)
            user_news_file = user_news_dir / f"{user_id}_{timestamp}.json"
            with open(user_news_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "user_id": user_id,
                    "profession": profession,
                    "date": timestamp,
                    "items": news_items,
                    "created_at": datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            print(f"   å·²ä¿å­˜ç”¨æˆ·æ–°é—»: {user_news_file}")
        except Exception as save_error:
            print(f"   ä¿å­˜ç”¨æˆ·æ–°é—»å¤±è´¥: {save_error}")
        
        return {
            "items": news_items,
            "profession": profession,
            "source": "tavily_ai",
            "count": len(news_items)
        }
        
    except Exception as e:
        print(f"âŒ [News] ç”Ÿæˆå¤±è´¥: {e}")
        # é™çº§åˆ° mock æ•°æ®
        return {
            "items": [],
            "error": str(e),
            "source": "error"
        }


@router.get("/user-daily-news/{user_id}")
async def get_user_daily_news(user_id: str):
    """
    è·å–ç”¨æˆ·ä»Šæ—¥å·²ç”Ÿæˆçš„æ–°é—»
    - ç”¨äºé¡µé¢åˆ·æ–°åæ¢å¤ç”¨æˆ·çš„å†…å®¹
    - å…è´¹ç”¨æˆ·æ¯å¤©åªèƒ½çœ‹åˆ°å›ºå®šçš„å†…å®¹
    """
    from datetime import datetime
    
    today = datetime.now().strftime("%Y-%m-%d")
    user_news_dir = DATA_DIR / "user_news"
    user_news_file = user_news_dir / f"{user_id}_{today}.json"
    
    if user_news_file.exists():
        try:
            with open(user_news_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"ğŸ“¦ [News] è¿”å›ç”¨æˆ· {user_id} çš„ä»Šæ—¥æ–°é—»")
                return {
                    "items": data.get("items", []),
                    "profession": data.get("profession", ""),
                    "date": data.get("date", today),
                    "source": "user_daily_cache"
                }
        except Exception as e:
            print(f"è¯»å–ç”¨æˆ·æ–°é—»å¤±è´¥: {e}")
    
    return {
        "items": [],
        "source": "not_found"
    }


# ============================================
# ç”Ÿæˆé€šç”¨AIæ–°é—»ï¼ˆä¸å…³è”ç”¨æˆ·èŒä¸šï¼‰
# ============================================
@router.get("/generate-general")
async def generate_general_news(
    user_id: str = Query("anonymous", description="ç”¨æˆ·ID")
):
    """
    ç”Ÿæˆé€šç”¨ AI æ–°é—» - ä¸å…³è”ç”¨æˆ·èŒä¸šï¼Œé€‚åˆæ‰€æœ‰äººé˜…è¯»
    """
    from services.ai_processor import ai_processor
    from services.content_safety import check_rate_limit, is_user_blocked
    import uuid
    import asyncio
    from tavily import TavilyClient
    from config import get_settings
    
    # å®‰å…¨æ£€æµ‹
    if is_user_blocked(user_id):
        raise HTTPException(status_code=403, detail="è´¦å·å·²è¢«é™åˆ¶ï¼Œè¯·è”ç³»ç®¡ç†å‘˜")
    
    rate_ok, rate_msg = check_rate_limit(user_id)
    if not rate_ok:
        raise HTTPException(status_code=429, detail=rate_msg)
    
    print(f"ğŸ”„ [GeneralNews] å¼€å§‹ç”Ÿæˆé€šç”¨AIæ–°é—»")
    
    try:
        # è·å– Tavily API Key
        keys = get_settings().get_tavily_keys()
        if not keys:
            raise Exception("æœªé…ç½® Tavily API Key")
        
        api_key = keys[0]
        
        # æœç´¢æœ€æ–° AI æ–°é—» - å›½å†…å¤–åˆ†å¼€æœç´¢
        from datetime import datetime, timedelta
        today = datetime.now()
        date_str = today.strftime("%Yå¹´%mæœˆ")
        
        # å›½å†…æ–°é—»æœç´¢å…³é”®è¯
        cn_query_templates = [
            f"AIäººå·¥æ™ºèƒ½ é‡å¤§çªç ´ {date_str}",
            f"AIå¤§æ¨¡å‹ é‡ç£…å‘å¸ƒ {date_str}",
            f"DeepSeek å­—èŠ‚è±†åŒ… ç™¾åº¦æ–‡å¿ƒ é˜¿é‡Œé€šä¹‰ æ–°é—»",
            f"AI Agent æ™ºèƒ½ä½“ æœ€æ–°çªç ´ {date_str}",
        ]
        # å›½é™…æ–°é—»æœç´¢å…³é”®è¯
        en_query_templates = [
            "AI artificial intelligence breakthrough news",
            "OpenAI Anthropic Google AI major update",
            "ChatGPT Claude Gemini latest news",
            "AI technology innovation news today",
        ]
        
        import random
        cn_query = random.choice(cn_query_templates)
        en_query = random.choice(en_query_templates)
        print(f"   å›½å†…æœç´¢: {cn_query}")
        print(f"   å›½é™…æœç´¢: {en_query}")
        
        # ä¸­æ–‡ç½‘ç«™ï¼ˆä¸å«çŸ¥ä¹ï¼‰
        cn_domains = [
            "36kr.com", "sspai.com", "juejin.cn",
            "mp.weixin.qq.com", "csdn.net", "jiqizhixin.com",
            "pingwest.com", "geekpark.net", "leiphone.com",
        ]
        # å›½é™…ç§‘æŠ€åª’ä½“
        en_domains = [
            "theverge.com", "techcrunch.com", "wired.com",
            "arstechnica.com", "venturebeat.com", "zdnet.com",
            "cnet.com", "engadget.com", "thenextweb.com",
            "reuters.com", "bbc.com", "nature.com", "ieee.org",
            "openai.com", "anthropic.com", "huggingface.co",
            "towardsdatascience.com", "medium.com"
        ]
        
        client = TavilyClient(api_key=api_key)
        
        # æœç´¢å›½å†…æ–°é—»
        cn_response = await asyncio.to_thread(
            client.search,
            query=cn_query,
            search_depth="advanced",
            max_results=15,
            include_domains=cn_domains,
            days=3
        )
        cn_results = cn_response.get("results", [])
        
        # æœç´¢å›½é™…æ–°é—»
        en_response = await asyncio.to_thread(
            client.search,
            query=en_query,
            search_depth="advanced",
            max_results=15,
            include_domains=en_domains,
            days=3
        )
        en_results = en_response.get("results", [])
        
        # åˆå¹¶ç»“æœ
        results = cn_results + en_results
        
        if not results:
            raise Exception("æœªæœç´¢åˆ°æœ‰æ•ˆæ–°é—»")
        
        print(f"   æœç´¢åˆ° {len(results)} æ¡ç»“æœ")
        
        # æ ¼å¼åŒ–æœç´¢ç»“æœä¾› AI å¤„ç†
        search_context = ""
        for i, res in enumerate(results):
            pub_date = res.get('published_date', '') or res.get('publishedDate', '') or 'æœªçŸ¥'
            search_context += f"[{i+1}] æ ‡é¢˜: {res.get('title', '')}\né“¾æ¥: {res.get('url', '')}\nå‘å¸ƒæ—¶é—´: {pub_date}\næ‘˜è¦: {res.get('content', '')}\n\n"
        
        # AI ç”Ÿæˆç»“æ„åŒ–æ–°é—»å¡ç‰‡ï¼ˆé€šç”¨ç‰ˆï¼Œä¸å…³è”èŒä¸šï¼‰
        today_str = today.strftime("%Yå¹´%mæœˆ%dæ—¥")
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI è¡Œä¸šåˆ†æå¸ˆã€‚ä»Šå¤©æ˜¯ {today_str}ã€‚
è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹æœç´¢ç»“æœï¼Œç”Ÿæˆ 10 æ¡é«˜è´¨é‡çš„ AI è¡Œä¸šæ–°é—»ç®€æŠ¥ã€‚

æœç´¢ç»“æœï¼š
{search_context}

è¦æ±‚ï¼š
1. ã€æ—¶æ•ˆæ€§ä¼˜å…ˆã€‘ä¼˜å…ˆé€‰æ‹©å‘å¸ƒæ—¶é—´æœ€è¿‘çš„æ–°é—»
2. ã€å›½å†…å¤–å¹³è¡¡ã€‘å¿…é¡»åŒæ—¶åŒ…å«å›½å†…å’Œå›½é™…æ–°é—»ï¼Œå¤§çº¦å„å ä¸€åŠ
3. æ¯æ¡æ–°é—»éƒ½å¿…é¡»åŸºäºçœŸå®çš„æœç´¢ç»“æœï¼Œä¸è¦ç¼–é€ 
4. ã€é‡è¦ã€‘æ¯æ¡æ–°é—»å¿…é¡»ä½¿ç”¨ä¸åŒçš„åŸæ–‡é“¾æ¥(url)ï¼Œç»å¯¹ä¸èƒ½é‡å¤ï¼
5. url å­—æ®µå¿…é¡»ç›´æ¥å¤åˆ¶æœç´¢ç»“æœä¸­çš„"é“¾æ¥"ï¼Œä¸è¦ä¿®æ”¹
6. æ‘˜è¦è¦ç®€æ´æœ‰ä¿¡æ¯é‡ï¼ˆ80-120å­—ï¼‰ï¼Œè‹±æ–‡æ–°é—»è¯·ç¿»è¯‘æˆä¸­æ–‡
7. é‡è¦æ€§åˆ†æè¦è¯´æ˜è¿™æ¡æ–°é—»ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨
8. æ¨èè¡ŒåŠ¨è¦ç»™å‡ºå…·ä½“å¯æ“ä½œçš„å»ºè®®

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼š
[
  {{
    "id": "news-1",
    "title": "æ–°é—»æ ‡é¢˜ï¼ˆç®€æ´æœ‰åŠ›ï¼‰",
    "tags": ["#æ ‡ç­¾1", "#æ ‡ç­¾2", "#æ ‡ç­¾3"],
    "summary": "æ–°é—»æ‘˜è¦ï¼Œç®€æ´æœ‰ä¿¡æ¯é‡ï¼ŒåŒ…å«æ—¶é—´ä¿¡æ¯",
    "impact": "è¿™æ¡æ–°é—»ä¸ºä»€ä¹ˆé‡è¦ï¼Œå¯¹æ™®é€šäººæœ‰ä»€ä¹ˆå½±å“",
    "prompt": "ä¸€ä¸ªé€šç”¨çš„AIä½¿ç”¨å»ºè®®æˆ–å¯å¤åˆ¶çš„Prompt",
    "url": "ç›´æ¥å¤åˆ¶æœç´¢ç»“æœä¸­çš„é“¾æ¥"
  }}
]

åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        import json as json_module
        response = await asyncio.to_thread(
            ai_processor.client.chat.completions.create,
            model=ai_processor.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=6000  # å¢åŠ ä»¥æ”¯æŒ10æ¡æ–°é—»
        )
        
        content = response.choices[0].message.content.strip()
        
        # æå– JSON
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].strip()
        
        start = content.find('[')
        end = content.rfind(']')
        if start != -1 and end != -1:
            content = content[start:end+1]
        
        news_items = json_module.loads(content)
        original_count = len(news_items)
        
        # å»é™¤é‡å¤ URL çš„æ–°é—»
        seen_urls = set()
        unique_items = []
        for item in news_items:
            url = item.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_items.append(item)
        news_items = unique_items
        
        if len(news_items) < original_count:
            print(f"âš ï¸ [GeneralNews] å»é™¤äº† {original_count - len(news_items)} æ¡é‡å¤ URL çš„æ–°é—»")
        
        # æ·»åŠ å”¯ä¸€ ID å’Œæ—¶é—´æˆ³
        timestamp = today.strftime("%Y-%m-%d")
        for item in news_items:
            item['id'] = f"general-{uuid.uuid4().hex[:8]}"
            item['timestamp'] = timestamp
        
        print(f"âœ… [GeneralNews] æˆåŠŸç”Ÿæˆ {len(news_items)} æ¡é€šç”¨æ–°é—»")
        
        # ä¿å­˜é€šç”¨æ–°é—»åˆ°æ–‡ä»¶
        try:
            general_news_dir = DATA_DIR / "general_news"
            general_news_dir.mkdir(parents=True, exist_ok=True)
            general_news_file = general_news_dir / f"{user_id}_{timestamp}.json"
            with open(general_news_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "user_id": user_id,
                    "date": timestamp,
                    "items": news_items,
                    "created_at": datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            print(f"   å·²ä¿å­˜é€šç”¨æ–°é—»: {general_news_file}")
        except Exception as save_error:
            print(f"   ä¿å­˜é€šç”¨æ–°é—»å¤±è´¥: {save_error}")
        
        return {
            "items": news_items,
            "source": "tavily_ai_general",
            "count": len(news_items)
        }
        
    except Exception as e:
        print(f"âŒ [GeneralNews] ç”Ÿæˆå¤±è´¥: {e}")
        return {
            "items": [],
            "error": str(e),
            "source": "error"
        }


@router.get("/user-daily-general-news/{user_id}")
async def get_user_daily_general_news(user_id: str):
    """
    è·å–ç”¨æˆ·ä»Šæ—¥å·²ç”Ÿæˆçš„é€šç”¨æ–°é—»
    """
    from datetime import datetime
    
    today = datetime.now().strftime("%Y-%m-%d")
    general_news_dir = DATA_DIR / "general_news"
    general_news_file = general_news_dir / f"{user_id}_{today}.json"
    
    if general_news_file.exists():
        try:
            with open(general_news_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"ğŸ“¦ [GeneralNews] è¿”å›ç”¨æˆ· {user_id} çš„ä»Šæ—¥é€šç”¨æ–°é—»")
                return {
                    "items": data.get("items", []),
                    "date": data.get("date", today),
                    "source": "user_daily_cache"
                }
        except Exception as e:
            print(f"è¯»å–é€šç”¨æ–°é—»å¤±è´¥: {e}")
    
    return {
        "items": [],
        "source": "not_found"
    }


@router.get("/{insight_id}", response_model=InsightCard)
async def get_insight_detail(insight_id: str):
    """Get a single insight card by ID."""
    insight = await storage.get_insight_by_id(insight_id)
    
    if not insight:
        raise HTTPException(status_code=404, detail="Insight not found")
    
    return InsightCard(**insight)


class PersonalizeRequest(BaseModel):
    profession: str = "èŒåœºäººå£«"  # ç”¨æˆ·è®¾ç½®çš„èŒä¸š
    news: dict  # åŒ…å« id, title, summary, url, tags


@router.post("/personalize")
async def get_personalized_insight(request: PersonalizeRequest):
    """
    ä¸ºæŒ‡å®šæ–°é—»ç”Ÿæˆä¸ªæ€§åŒ–è§£è¯»ï¼ˆåŸºäºç”¨æˆ·èŒä¸šï¼‰
    """
    from services.ai_processor import ai_processor
    from models import RawNews
    
    profession = request.profession
    news_data = request.news
    
    # æ„å»º RawNews å¯¹è±¡
    from datetime import datetime
    news = RawNews(
        id=news_data.get('id', ''),
        source_url=news_data.get('url', ''),
        source_name="FocusAI",
        title=news_data.get('title', ''),
        content=news_data.get('summary', ''),
        published_at=datetime.now(),
        created_at=datetime.now()
    )
    
    # æ„å»ºç®€å•çš„ç”¨æˆ·ç”»åƒï¼ˆåªåŒ…å«èŒä¸šï¼‰
    simple_profile = {"profession": profession}
    
    # ç”Ÿæˆä¸ªæ€§åŒ–è§£è¯»
    personalized = await ai_processor.generate_personalized_insight(news, simple_profile)
    
    if personalized:
        return {
            "success": True,
            "insight": personalized.model_dump(),
            "profession": profession
        }
    else:
        raise HTTPException(status_code=500, detail="ç”Ÿæˆä¸ªæ€§åŒ–è§£è¯»å¤±è´¥")

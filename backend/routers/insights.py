"""
FocusAI Insights API Router
Handles news card listing and detail endpoints.
"""
from fastapi import APIRouter, Query, HTTPException
from pydantic import BaseModel
from typing import List, Dict
from collections import defaultdict
import time

from models import InsightCard, InsightListResponse
from storage import storage
from config import PROFESSIONS

router = APIRouter(prefix="/api/insights", tags=["Insights"])


# ============================================
# ç¼“å­˜ç®¡ç†å™¨ - å‡å°‘ API è°ƒç”¨ï¼ŒåŠ å¿«å“åº”é€Ÿåº¦
# ============================================
class ContentCache:
    """
    å·¥å…·/æ¡ˆä¾‹ç¼“å­˜ç®¡ç†å™¨
    - æ¯æ¬¡ API è°ƒç”¨è·å– 12 æ¡ï¼Œæ˜¾ç¤º 6 æ¡ï¼Œç¼“å­˜ 6 æ¡
    - åˆ·æ–°æ—¶å…ˆä»ç¼“å­˜å–ï¼Œç¼“å­˜ç©ºäº†å†è°ƒç”¨ API
    - ç¼“å­˜ 30 åˆ†é’Ÿè¿‡æœŸ
    """
    DISPLAY_COUNT = 6      # æ¯æ¬¡æ˜¾ç¤ºæ•°é‡
    FETCH_COUNT = 18       # æ¯æ¬¡è·å–æ•°é‡ï¼ˆæ˜¾ç¤º6æ¡ + ç¼“å­˜12æ¡ = æ”¯æŒåˆ°2æ¬¡åˆ·æ–°ï¼‰
    CACHE_TTL = 1800       # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆ30åˆ†é’Ÿï¼‰
    
    def __init__(self):
        # ç¼“å­˜ç»“æ„: {profession: {"items": [...], "timestamp": ..., "seed": ...}}
        self.tools_cache: Dict[str, dict] = {}
        self.cases_cache: Dict[str, dict] = {}
    
    def _is_expired(self, cache_entry: dict) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
        if not cache_entry:
            return True
        return time.time() - cache_entry.get("timestamp", 0) > self.CACHE_TTL
    
    def get_tools(self, profession: str) -> tuple[list, bool]:
        """
        è·å–å·¥å…·ç¼“å­˜
        è¿”å›: (items, need_fetch) - ç¼“å­˜é¡¹å’Œæ˜¯å¦éœ€è¦è°ƒç”¨ API
        """
        cache = self.tools_cache.get(profession)
        
        # ç¼“å­˜ä¸å­˜åœ¨æˆ–è¿‡æœŸ
        if self._is_expired(cache):
            return [], True
        
        items = cache.get("items", [])
        
        # ç¼“å­˜ä¸è¶³ï¼Œéœ€è¦é‡æ–°è·å–
        if len(items) < self.DISPLAY_COUNT:
            return [], True
        
        # å–å‡ºå‰ 6 æ¡ï¼Œå‰©ä½™çš„ä¿ç•™åœ¨ç¼“å­˜
        result = items[:self.DISPLAY_COUNT]
        cache["items"] = items[self.DISPLAY_COUNT:]
        
        print(f"ğŸ“¦ [Tools] ç¼“å­˜å‘½ä¸­! è¿”å› {len(result)} æ¡ï¼Œå‰©ä½™ç¼“å­˜ {len(cache['items'])} æ¡")
        return result, False
    
    def set_tools(self, profession: str, items: list, seed: int):
        """è®¾ç½®å·¥å…·ç¼“å­˜"""
        self.tools_cache[profession] = {
            "items": items,
            "timestamp": time.time(),
            "seed": seed
        }
        print(f"ğŸ“¥ [Tools] ç¼“å­˜æ›´æ–°! å­˜å…¥ {len(items)} æ¡")
    
    def get_cases(self, profession: str) -> tuple[list, bool]:
        """è·å–æ¡ˆä¾‹ç¼“å­˜"""
        cache = self.cases_cache.get(profession)
        
        if self._is_expired(cache):
            return [], True
        
        items = cache.get("items", [])
        
        if len(items) < self.DISPLAY_COUNT:
            return [], True
        
        result = items[:self.DISPLAY_COUNT]
        cache["items"] = items[self.DISPLAY_COUNT:]
        
        print(f"ğŸ“¦ [Cases] ç¼“å­˜å‘½ä¸­! è¿”å› {len(result)} æ¡ï¼Œå‰©ä½™ç¼“å­˜ {len(cache['items'])} æ¡")
        return result, False
    
    def set_cases(self, profession: str, items: list, seed: int):
        """è®¾ç½®æ¡ˆä¾‹ç¼“å­˜"""
        self.cases_cache[profession] = {
            "items": items,
            "timestamp": time.time(),
            "seed": seed
        }
        print(f"ğŸ“¥ [Cases] ç¼“å­˜æ›´æ–°! å­˜å…¥ {len(items)} æ¡")
    
    def get_next_seed(self, cache_type: str, profession: str) -> int:
        """è·å–ä¸‹ä¸€ä¸ª seed å€¼"""
        cache = self.tools_cache if cache_type == "tools" else self.cases_cache
        entry = cache.get(profession, {})
        return entry.get("seed", -1) + 1


# å…¨å±€ç¼“å­˜å®ä¾‹
content_cache = ContentCache()


@router.get("", response_model=InsightListResponse)
async def list_insights(
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(20, ge=1, le=50, description="Items per page"),
):
    """
    Get a list of AI insight cards.
    Returns cards from local storage, ordered by newest first.
    """
    offset = (page - 1) * page_size
    items = await storage.get_insights(limit=page_size, offset=offset)
    total = await storage.get_insights_count()
    
    # è½¬æ¢ä¸º InsightCard å¯¹è±¡
    cards = [InsightCard(**item) for item in items]
    
    return InsightListResponse(
        items=cards,
        total=total,
        page=page,
        page_size=page_size
    )


@router.get("/mock", response_model=List[InsightCard])
async def get_mock_insights(
    profession: str = Query("other", description="User profession for mock data")
):
    """
    Get mock insight cards for development/demo.
    No database required.
    """
    profession_display = PROFESSIONS.get(profession, "èŒåœºäººå£«")
    
    mock_data = [
        InsightCard(
            id="mock-1",
            title="DeepSeek V3 å‘å¸ƒï¼šæ€§èƒ½è¶…è¶Š GPT-4",
            tags=["#DeepSeek", "#å¤§æ¨¡å‹", "#å¼€æº"],
            summary="æ·±åº¦æ±‚ç´¢å‘å¸ƒ DeepSeek V3ï¼Œåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Š GPT-4ï¼Œä¸”å®Œå…¨å¼€æºå…è´¹ã€‚æ¨¡å‹æ”¯æŒ 128K ä¸Šä¸‹æ–‡ï¼Œæ¨ç†é€Ÿåº¦æå‡ 3 å€ã€‚",
            impact=f"ä½œä¸º{profession_display}ï¼Œä½ å¯ä»¥ç”¨ DeepSeek V3 æ¥è¾…åŠ©æ—¥å¸¸å·¥ä½œã€‚å®ƒçš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ç‰¹åˆ«é€‚åˆå¤„ç†é•¿æ–‡æ¡£ã€ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šã€‚å»ºè®®ä½ å…ˆåœ¨ç®€å•ä»»åŠ¡ä¸Šè¯•ç”¨ï¼Œé€æ­¥æ›¿ä»£éƒ¨åˆ†é‡å¤æ€§å·¥ä½œã€‚",
            prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ã€‚è¯·å¸®æˆ‘åˆ†æä»¥ä¸‹å†…å®¹ï¼Œå¹¶ç»™å‡ºç»“æ„åŒ–çš„æ€»ç»“å’Œå¯è¡Œçš„å»ºè®®ï¼š\n\n[åœ¨æ­¤ç²˜è´´ä½ çš„å†…å®¹]",
            url="https://www.deepseek.com/",
            timestamp="2024-12-03"
        ),
        InsightCard(
            id="mock-2",
            title="Midjourney V6.1 é‡å¤§æ›´æ–°ï¼šæ–‡å­—æ¸²æŸ“èƒ½åŠ›çªç ´",
            tags=["#Midjourney", "#AIç»˜ç”»", "#è®¾è®¡"],
            summary="Midjourney å‘å¸ƒ V6.1 ç‰ˆæœ¬ï¼Œé¦–æ¬¡å®ç°é«˜è´¨é‡æ–‡å­—æ¸²æŸ“ï¼Œå¯ä»¥ç›´æ¥åœ¨å›¾ç‰‡ä¸­ç”Ÿæˆæ¸…æ™°å¯è¯»çš„æ–‡å­—ï¼ŒåŒæ—¶å›¾åƒç”Ÿæˆé€Ÿåº¦æå‡ 2 å€ã€‚",
            impact=f"å¯¹{profession_display}æ¥è¯´ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥å¿«é€Ÿç”Ÿæˆå¸¦æœ‰æ–‡å­—çš„æµ·æŠ¥ã€å°é¢ã€å®£ä¼ å›¾ï¼Œä¸å†éœ€è¦åæœŸ PS åŠ å­—ã€‚éå¸¸é€‚åˆåˆ¶ä½œæ•™å­¦ææ–™ã€è¯¾ä»¶é…å›¾ã€‚",
            prompt="/imagine prompt: æ•™è‚²ä¸»é¢˜æ’ç”»ï¼Œæ¸©é¦¨æ˜äº®çš„æ•™å®¤åœºæ™¯ï¼Œå­¦ç”Ÿè®¤çœŸå­¦ä¹ ï¼Œé»‘æ¿ä¸Šå†™ç€\"çŸ¥è¯†æ”¹å˜å‘½è¿\"ï¼Œæ¸©æš–çš„é˜³å…‰é€è¿‡çª—æˆ· --v 6.1 --ar 16:9",
            url="https://midjourney.com/",
            timestamp="2024-12-02"
        ),
        InsightCard(
            id="mock-3",
            title="Claude 3.5 Sonnet æ–°åŠŸèƒ½ï¼šArtifacts å®æ—¶é¢„è§ˆ",
            tags=["#Claude", "#Anthropic", "#ç¼–ç¨‹"],
            summary="Anthropic ä¸º Claude 3.5 Sonnet æ¨å‡º Artifacts åŠŸèƒ½ï¼Œç”¨æˆ·å¯ä»¥åœ¨å¯¹è¯ä¸­å®æ—¶é¢„è§ˆå’Œè¿è¡Œä»£ç ã€æŸ¥çœ‹ç”Ÿæˆçš„æ–‡æ¡£å’Œå›¾è¡¨ã€‚",
            impact=f"ä½œä¸º{profession_display}ï¼ŒArtifacts åŠŸèƒ½å¯ä»¥å¸®ä½ å¿«é€ŸéªŒè¯æƒ³æ³•ã€‚æ¯”å¦‚è®© Claude ç”Ÿæˆä¸€ä¸ªè¯¾ç¨‹å¤§çº²ï¼Œå®ƒä¼šä»¥ç»“æ„åŒ–çš„æ–¹å¼å‘ˆç°ï¼Œä½ å¯ä»¥ç›´æ¥ç¼–è¾‘å’Œå¯¼å‡ºã€‚",
            prompt="è¯·å¸®æˆ‘è®¾è®¡ä¸€ä¸ªä¸ºæœŸ 4 å‘¨çš„è¯¾ç¨‹å¤§çº²ï¼Œä¸»é¢˜æ˜¯ [ä½ çš„è¯¾ç¨‹ä¸»é¢˜]ã€‚è¦æ±‚ï¼š\n1. æ¯å‘¨ 2 èŠ‚è¯¾ï¼Œæ¯èŠ‚ 1 å°æ—¶\n2. åŒ…å«ç†è®ºè®²è§£å’Œå®è·µç»ƒä¹ \n3. è®¾ç½®è¯¾åä½œä¸šå’Œé˜¶æ®µæ€§æµ‹éªŒ\n4. ä½¿ç”¨ Markdown æ ¼å¼è¾“å‡º",
            url="https://claude.ai/",
            timestamp="2024-12-01"
        ),
        InsightCard(
            id="mock-4",
            title="Sora å³å°†å¼€æ”¾ï¼šAI è§†é¢‘ç”Ÿæˆè¿›å…¥å®ç”¨é˜¶æ®µ",
            tags=["#Sora", "#OpenAI", "#è§†é¢‘ç”Ÿæˆ"],
            summary="OpenAI å®£å¸ƒ Sora å°†äºæœ¬æœˆå‘ ChatGPT Plus ç”¨æˆ·å¼€æ”¾ã€‚Sora å¯ä»¥æ ¹æ®æ–‡å­—æè¿°ç”Ÿæˆæœ€é•¿ 60 ç§’çš„é«˜æ¸…è§†é¢‘ï¼Œæ”¯æŒå¤šç§é£æ ¼ã€‚",
            impact=f"è§†é¢‘å†…å®¹å°†å˜å¾—æ›´å®¹æ˜“åˆ¶ä½œã€‚{profession_display}å¯ä»¥ç”¨å®ƒæ¥åˆ¶ä½œæ•™å­¦æ¼”ç¤ºè§†é¢‘ã€è¯¾ç¨‹å®£ä¼ ç‰‡ã€‚è™½ç„¶ç›®å‰è¿˜ä¸èƒ½å®Œå…¨æ›¿ä»£ä¸“ä¸šåˆ¶ä½œï¼Œä½†è¶³ä»¥åº”å¯¹æ—¥å¸¸éœ€æ±‚ã€‚",
            prompt="Create a 15-second educational video: A friendly animated teacher explaining the concept of [ä½ çš„ä¸»é¢˜] to students. Warm, inviting classroom setting with soft lighting. Professional yet approachable style.",
            url="https://openai.com/sora",
            timestamp="2024-11-30"
        ),
    ]
    
    return mock_data


@router.get("/tools")
async def get_ai_tools(
    profession: str = Query("èŒåœºäººå£«", description="ç”¨æˆ·èŒä¸š"),
    force_refresh: bool = Query(False, description="å¼ºåˆ¶åˆ·æ–°ï¼Œè·³è¿‡ç¼“å­˜")
):
    """
    è·å– AI å·¥å…·æ¨è
    - é¦–å…ˆå°è¯•ä»ç¼“å­˜è·å–ï¼ˆç¬é—´è¿”å›ï¼‰
    - ç¼“å­˜ä¸è¶³æ—¶è°ƒç”¨ Tavily API
    """
    from services.ai_processor import ai_processor
    
    # 1. å°è¯•ä»ç¼“å­˜è·å–
    if not force_refresh:
        cached_items, need_fetch = content_cache.get_tools(profession)
        if not need_fetch:
            return {
                "items": cached_items, 
                "profession": profession, 
                "source": "cache",
                "cached": True
            }
    
    # 2. ç¼“å­˜ä¸è¶³ï¼Œè°ƒç”¨ API
    try:
        seed = content_cache.get_next_seed("tools", profession)
        print(f"ğŸ”„ [Tools] ç¼“å­˜ä¸è¶³ï¼Œè°ƒç”¨ API (seed={seed})")
        
        # è·å– 12 æ¡ç»“æœ
        tools = await ai_processor.search_and_recommend_tools(
            profession, 
            refresh_seed=seed,
            result_count=ContentCache.FETCH_COUNT  # è·å– 12 æ¡
        )
        
        # è¿”å›å‰ 6 æ¡ï¼Œå‰©ä½™çš„å­˜å…¥ç¼“å­˜
        display_items = tools[:ContentCache.DISPLAY_COUNT]
        cache_items = tools[ContentCache.DISPLAY_COUNT:]
        
        if cache_items:
            content_cache.set_tools(profession, cache_items, seed)
        
        return {
            "items": display_items, 
            "profession": profession, 
            "source": "web_search",
            "cached": False,
            "total_fetched": len(tools)
        }
    except Exception as e:
        print(f"Error searching tools: {e}")
        # é™çº§åˆ°é™æ€æ•°æ®
        import json
        from pathlib import Path
        tools_file = Path(__file__).parent.parent / "data" / "ai_tools.json"
        if tools_file.exists():
            with open(tools_file, 'r', encoding='utf-8') as f:
                return {"items": json.load(f)[:6], "source": "fallback"}
        return {"items": [], "error": str(e)}


@router.get("/cases")
async def get_ai_cases(
    profession: str = Query("èŒåœºäººå£«", description="ç”¨æˆ·èŒä¸š"),
    force_refresh: bool = Query(False, description="å¼ºåˆ¶åˆ·æ–°ï¼Œè·³è¿‡ç¼“å­˜")
):
    """
    è·å– AI å®æˆ˜æ¡ˆä¾‹
    - é¦–å…ˆå°è¯•ä»ç¼“å­˜è·å–ï¼ˆç¬é—´è¿”å›ï¼‰
    - ç¼“å­˜ä¸è¶³æ—¶è°ƒç”¨ Tavily API
    """
    from services.ai_processor import ai_processor
    
    # 1. å°è¯•ä»ç¼“å­˜è·å–
    if not force_refresh:
        cached_items, need_fetch = content_cache.get_cases(profession)
        if not need_fetch:
            return {
                "items": cached_items, 
                "profession": profession, 
                "source": "cache",
                "cached": True
            }
    
    # 2. ç¼“å­˜ä¸è¶³ï¼Œè°ƒç”¨ API
    try:
        seed = content_cache.get_next_seed("cases", profession)
        print(f"ğŸ”„ [Cases] ç¼“å­˜ä¸è¶³ï¼Œè°ƒç”¨ API (seed={seed})")
        
        # è·å– 12 æ¡ç»“æœ
        cases = await ai_processor.search_and_recommend_cases(
            profession, 
            refresh_seed=seed,
            result_count=ContentCache.FETCH_COUNT  # è·å– 12 æ¡
        )
        
        # è¿”å›å‰ 6 æ¡ï¼Œå‰©ä½™çš„å­˜å…¥ç¼“å­˜
        display_items = cases[:ContentCache.DISPLAY_COUNT]
        cache_items = cases[ContentCache.DISPLAY_COUNT:]
        
        if cache_items:
            content_cache.set_cases(profession, cache_items, seed)
        
        return {
            "items": display_items, 
            "profession": profession, 
            "source": "web_search",
            "cached": False,
            "total_fetched": len(cases)
        }
    except Exception as e:
        print(f"Error searching cases: {e}")
        # é™çº§åˆ°é™æ€æ•°æ®
        import json
        from pathlib import Path
        cases_file = Path(__file__).parent.parent / "data" / "ai_cases.json"
        if cases_file.exists():
            with open(cases_file, 'r', encoding='utf-8') as f:
                return {"items": json.load(f)[:6], "source": "fallback"}
        return {"items": [], "error": str(e)}


# ============================================
# ç”Ÿæˆä»Šæ—¥æ–°é—» - ä½¿ç”¨ Tavily æœç´¢æœ€æ–° AI èµ„è®¯
# ============================================
@router.get("/generate")
async def generate_daily_news(
    profession: str = Query("èŒåœºäººå£«", description="ç”¨æˆ·èŒä¸š"),
    user_id: str = Query("anonymous", description="ç”¨æˆ·ID")
):
    """
    ç”Ÿæˆä»Šæ—¥ AI æ–°é—» - æœç´¢æœ€æ–°èµ„è®¯å¹¶ç”¨ AI ç”Ÿæˆä¸ªæ€§åŒ–è§£è¯»
    """
    from services.ai_processor import ai_processor
    from services.content_safety import validate_profession, check_rate_limit, is_user_blocked
    import uuid
    import asyncio
    from tavily import TavilyClient
    from config import get_settings
    
    # å®‰å…¨æ£€æµ‹
    if is_user_blocked(user_id):
        raise HTTPException(status_code=403, detail="è´¦å·å·²è¢«é™åˆ¶ï¼Œè¯·è”ç³»ç®¡ç†å‘˜")
    
    rate_ok, rate_msg = check_rate_limit(user_id)
    if not rate_ok:
        raise HTTPException(status_code=429, detail=rate_msg)
    
    valid, msg = validate_profession(profession, user_id)
    if not valid:
        raise HTTPException(status_code=400, detail=msg)
    
    print(f"ğŸ”„ [News] å¼€å§‹ç”Ÿæˆä»Šæ—¥æ–°é—»ï¼ŒèŒä¸š: {profession}")
    
    try:
        # è·å– Tavily API Key
        keys = get_settings().get_tavily_keys()
        if not keys:
            raise Exception("æœªé…ç½® Tavily API Key")
        
        api_key = keys[0]
        
        # æœç´¢æœ€æ–° AI æ–°é—»ï¼ˆä¸­æ–‡ï¼‰- ä½¿ç”¨ 2025 å¹´å…³é”®è¯
        query_templates = [
            "AIäººå·¥æ™ºèƒ½ æœ€æ–°åŠ¨æ€ 2025",
            "AIå¤§æ¨¡å‹ æœ€æ–°å‘å¸ƒ 2025",
            "äººå·¥æ™ºèƒ½ è¡Œä¸šæ–°é—» æœ€æ–°",
            "AIå·¥å…· æ–°åŠŸèƒ½ å‘å¸ƒ 2025",
            "ChatGPT Claude Gemini æœ€æ–°æ¶ˆæ¯",
            "AI Agent æ™ºèƒ½ä½“ æœ€æ–°è¿›å±•",
        ]
        
        import random
        query = random.choice(query_templates)
        print(f"   æœç´¢æŸ¥è¯¢: {query}")
        
        # ä¸­æ–‡ç½‘ç«™ + å›½é™…å¯è®¿é—®ç½‘ç«™
        allowed_domains = [
            # ä¸­æ–‡ç½‘ç«™
            "zhihu.com", "36kr.com", "sspai.com", "juejin.cn",
            "mp.weixin.qq.com", "bilibili.com", "csdn.net",
            "jianshu.com", "woshipm.com", "jiqizhixin.com",
            "pingwest.com", "geekpark.net", "leiphone.com",
            # å›½é™…ç§‘æŠ€åª’ä½“ï¼ˆä¸­å›½å¯è®¿é—®ï¼‰
            "theverge.com", "techcrunch.com", "wired.com",
            "arstechnica.com", "venturebeat.com", "zdnet.com",
            "cnet.com", "engadget.com", "thenextweb.com",
            "reuters.com", "nature.com", "ieee.org",
            "mit.edu", "stanford.edu", "openai.com",
            "anthropic.com", "huggingface.co"
        ]
        
        client = TavilyClient(api_key=api_key)
        response = await asyncio.to_thread(
            client.search,
            query=query,
            search_depth="basic",
            max_results=15,
            include_domains=allowed_domains,
            days=3  # é™åˆ¶ä¸ºæœ€è¿‘3å¤©çš„å†…å®¹
        )
        
        results = response.get("results", [])
        if not results:
            raise Exception("æœªæœç´¢åˆ°æœ‰æ•ˆæ–°é—»")
        
        print(f"   æœç´¢åˆ° {len(results)} æ¡ç»“æœ")
        
        # æ ¼å¼åŒ–æœç´¢ç»“æœä¾› AI å¤„ç†
        search_context = ""
        for i, res in enumerate(results):
            search_context += f"[{i+1}] æ ‡é¢˜: {res.get('title', '')}\né“¾æ¥: {res.get('url', '')}\næ‘˜è¦: {res.get('content', '')}\n\n"
        
        # AI ç”Ÿæˆç»“æ„åŒ–æ–°é—»å¡ç‰‡
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI è¡Œä¸šåˆ†æå¸ˆã€‚æˆ‘ä¸ºä½ æœé›†äº†ä¸€äº›æœ€æ–°çš„ AI ç›¸å…³æ–°é—»ã€‚
è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹æœç´¢ç»“æœï¼Œå¹¶ä¸º"{profession}"ç”Ÿæˆ 6 æ¡é«˜è´¨é‡çš„ AI è¡Œä¸šæ´å¯Ÿå¡ç‰‡ã€‚

æœç´¢ç»“æœï¼š
{search_context}

è¦æ±‚ï¼š
1. æ¯æ¡æ´å¯Ÿéƒ½è¦åŸºäºçœŸå®çš„æœç´¢ç»“æœï¼Œä¸è¦ç¼–é€ 
2. ä¸ºæ¯æ¡æ–°é—»ç”Ÿæˆï¼šæ ‡é¢˜ã€æ ‡ç­¾ã€æ‘˜è¦ã€å¯¹è¯¥èŒä¸šçš„å…·ä½“å½±å“ã€å¯ç›´æ¥ä½¿ç”¨çš„ Prompt
3. æ‘˜è¦è¦ç®€æ´æœ‰ä¿¡æ¯é‡ï¼ˆ50-100å­—ï¼‰
4. å½±å“åˆ†æè¦é’ˆå¯¹ {profession} è¿™ä¸ªèŒä¸šå…·ä½“åŒ–
5. Prompt è¦å®ç”¨ï¼Œå¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼š
[
  {{
    "id": "news-1",
    "title": "æ–°é—»æ ‡é¢˜",
    "tags": ["#æ ‡ç­¾1", "#æ ‡ç­¾2", "#æ ‡ç­¾3"],
    "summary": "æ–°é—»æ‘˜è¦ï¼Œç®€æ´æœ‰ä¿¡æ¯é‡",
    "impact": "å¯¹{profession}çš„å…·ä½“å½±å“å’Œå»ºè®®",
    "prompt": "å¯ç›´æ¥ä½¿ç”¨çš„ Prompt ç¤ºä¾‹",
    "url": "åŸæ–‡é“¾æ¥"
  }}
]

åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        import json as json_module
        response = await asyncio.to_thread(
            ai_processor.client.chat.completions.create,
            model=ai_processor.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=4000
        )
        
        content = response.choices[0].message.content.strip()
        
        # æå– JSON
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].strip()
        
        start = content.find('[')
        end = content.rfind(']')
        if start != -1 and end != -1:
            content = content[start:end+1]
        
        news_items = json_module.loads(content)
        
        # æ·»åŠ å”¯ä¸€ ID å’Œæ—¶é—´æˆ³
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y-%m-%d")
        for item in news_items:
            item['id'] = f"news-{uuid.uuid4().hex[:8]}"
            item['timestamp'] = timestamp
        
        print(f"âœ… [News] æˆåŠŸç”Ÿæˆ {len(news_items)} æ¡æ–°é—»")
        
        return {
            "items": news_items,
            "profession": profession,
            "source": "tavily_ai",
            "count": len(news_items)
        }
        
    except Exception as e:
        print(f"âŒ [News] ç”Ÿæˆå¤±è´¥: {e}")
        # é™çº§åˆ° mock æ•°æ®
        return {
            "items": [],
            "error": str(e),
            "source": "error"
        }


@router.get("/{insight_id}", response_model=InsightCard)
async def get_insight_detail(insight_id: str):
    """Get a single insight card by ID."""
    insight = await storage.get_insight_by_id(insight_id)
    
    if not insight:
        raise HTTPException(status_code=404, detail="Insight not found")
    
    return InsightCard(**insight)


class PersonalizeRequest(BaseModel):
    profession: str = "èŒåœºäººå£«"  # ç”¨æˆ·è®¾ç½®çš„èŒä¸š
    news: dict  # åŒ…å« id, title, summary, url, tags


@router.post("/personalize")
async def get_personalized_insight(request: PersonalizeRequest):
    """
    ä¸ºæŒ‡å®šæ–°é—»ç”Ÿæˆä¸ªæ€§åŒ–è§£è¯»ï¼ˆåŸºäºç”¨æˆ·èŒä¸šï¼‰
    """
    from services.ai_processor import ai_processor
    from models import RawNews
    
    profession = request.profession
    news_data = request.news
    
    # æ„å»º RawNews å¯¹è±¡
    from datetime import datetime
    news = RawNews(
        id=news_data.get('id', ''),
        source_url=news_data.get('url', ''),
        source_name="FocusAI",
        title=news_data.get('title', ''),
        content=news_data.get('summary', ''),
        published_at=datetime.now(),
        created_at=datetime.now()
    )
    
    # æ„å»ºç®€å•çš„ç”¨æˆ·ç”»åƒï¼ˆåªåŒ…å«èŒä¸šï¼‰
    simple_profile = {"profession": profession}
    
    # ç”Ÿæˆä¸ªæ€§åŒ–è§£è¯»
    personalized = await ai_processor.generate_personalized_insight(news, simple_profile)
    
    if personalized:
        return {
            "success": True,
            "insight": personalized.model_dump(),
            "profession": profession
        }
    else:
        raise HTTPException(status_code=500, detail="ç”Ÿæˆä¸ªæ€§åŒ–è§£è¯»å¤±è´¥")

"""
FocusAI News Crawler Runner
Standalone script to crawl news and generate insights.

Usage:
    python run_crawler.py              # Crawl once
    python run_crawler.py --schedule   # Run with scheduler
"""
import asyncio
import argparse
from datetime import datetime
from apscheduler.schedulers.asyncio import AsyncIOScheduler

from config import get_settings
from storage import storage
from services.news_crawler import news_crawler
from services.ai_processor import ai_processor
from models import Profession, RawNews


async def crawl_and_process():
    """
    Main crawl and process pipeline:
    1. Crawl news from all sources
    2. Store raw news in database
    3. Generate insights using AI
    4. Store insights in database
    """
    print(f"\n{'='*50}")
    print(f"ğŸ• Starting crawl at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*50}\n")
    
    # Step 1: Crawl news
    print("ğŸ“¡ Step 1: Crawling news sources...")
    raw_news_list = await news_crawler.crawl_all()
    print(f"   Found {len(raw_news_list)} news items\n")
    
    # Step 2: Store raw news
    print("ğŸ’¾ Step 2: Storing raw news...")
    new_count = 0
    for news in raw_news_list:
        success = await storage.save_news(news)
        if success:
            new_count += 1
    print(f"   Stored {new_count} new items (skipped {len(raw_news_list) - new_count} duplicates)\n")
    
    # Step 3: Process unprocessed news with AI
    print("ğŸ¤– Step 3: Generating insights with AI...")
    unprocessed = await storage.get_unprocessed_news(limit=5)
    print(f"   Found {len(unprocessed)} unprocessed news items")
    
    for news_dict in unprocessed:
        title = news_dict.get('title', '')[:40]
        print(f"   Processing: {title}...")
        
        # Convert dict to RawNews object
        news = RawNews(
            id=news_dict.get('id'),
            source_url=news_dict.get('source_url'),
            source_name=news_dict.get('source_name'),
            title=news_dict.get('title'),
            content=news_dict.get('content'),
            published_at=news_dict.get('published_at'),
            created_at=news_dict.get('created_at')
        )
        
        # Generate general insight
        insight = await ai_processor.generate_general_insight(news)
        
        if insight:
            await storage.save_insight(insight)
            await storage.mark_news_processed(news.id)
            print(f"   âœ“ Generated insight")
        else:
            print(f"   âœ— Failed to generate insight")
    
    print(f"\n{'='*50}")
    print(f"âœ… Crawl completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*50}\n")


async def run_scheduler():
    """Run crawler on a schedule."""
    settings = get_settings()
    
    scheduler = AsyncIOScheduler()
    scheduler.add_job(
        crawl_and_process,
        'interval',
        hours=settings.crawl_interval_hours,
        id='news_crawler'
    )
    
    print(f"ğŸ“… Scheduler started. Crawling every {settings.crawl_interval_hours} hours.")
    print("   Press Ctrl+C to stop.\n")
    
    # Run once immediately
    await crawl_and_process()
    
    scheduler.start()
    
    # Keep running
    try:
        while True:
            await asyncio.sleep(3600)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Scheduler stopped.")
        scheduler.shutdown()


async def test_ai_processor():
    """Test the AI processor with a sample news item."""
    from models import RawNews
    import uuid
    
    print("ğŸ§ª Testing AI Processor...\n")
    
    # Create a sample news item
    sample_news = RawNews(
        id=str(uuid.uuid4()),
        source_url="https://example.com/test",
        source_name="Test Source",
        title="DeepSeek V3 å‘å¸ƒï¼šå¼€æºå¤§æ¨¡å‹æ–°æ ‡æ†",
        content="""
        æ·±åº¦æ±‚ç´¢ï¼ˆDeepSeekï¼‰ä»Šæ—¥å‘å¸ƒ DeepSeek V3 æ¨¡å‹ï¼Œè¿™æ˜¯ç›®å‰æœ€å¼ºå¤§çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ä¹‹ä¸€ã€‚
        
        ä¸»è¦ç‰¹ç‚¹ï¼š
        - åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Š GPT-4
        - æ”¯æŒ 128K è¶…é•¿ä¸Šä¸‹æ–‡
        - å®Œå…¨å¼€æºï¼Œå¯æœ¬åœ°éƒ¨ç½²
        - æ¨ç†é€Ÿåº¦æå‡ 3 å€
        
        DeepSeek V3 é‡‡ç”¨äº†å…¨æ–°çš„ MoEï¼ˆæ··åˆä¸“å®¶ï¼‰æ¶æ„ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å¤§å¹…é™ä½äº†è®¡ç®—æˆæœ¬ã€‚
        æ¨¡å‹å·²åœ¨ HuggingFace ä¸Šå¼€æºï¼Œå¼€å‘è€…å¯ä»¥å…è´¹ä¸‹è½½ä½¿ç”¨ã€‚
        """,
        published_at=datetime.now(),
        created_at=datetime.now()
    )
    
    # Test with different professions
    test_professions = [
        Profession.ONLINE_TEACHER,
        Profession.PRODUCT_MANAGER,
        Profession.FULLSTACK_ENGINEER,
    ]
    
    for profession in test_professions:
        print(f"\n--- Testing for: {profession.value} ---")
        insight = await ai_processor.generate_insight(sample_news, profession)
        
        if insight:
            print(f"Title: {insight.title}")
            print(f"Tags: {insight.tags}")
            print(f"Summary: {insight.summary[:100]}...")
            print(f"Impact: {insight.impact[:100]}...")
            print(f"Prompt: {insight.prompt[:80]}...")
        else:
            print("Failed to generate insight")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="FocusAI News Crawler")
    parser.add_argument("--schedule", action="store_true", help="Run with scheduler")
    parser.add_argument("--test", action="store_true", help="Test AI processor")
    args = parser.parse_args()
    
    if args.test:
        asyncio.run(test_ai_processor())
    elif args.schedule:
        asyncio.run(run_scheduler())
    else:
        asyncio.run(crawl_and_process())
